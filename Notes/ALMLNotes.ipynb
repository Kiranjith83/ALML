{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI ML notes\n",
    "| Ref: Reference: https://ml-course.github.io/master/notebooks/01%20-%20Introduction.html\n",
    "#### AI\n",
    "- AI in reality is in place from mid 60s, like spam filters, AVs \n",
    "- The compute power needed to make the AI to work was not available before 90s, but by 2011s advancement of computer power the AI is possible.\n",
    "- AI is very broad object.\n",
    "- - ML is a subset of AI \n",
    "- - - Deep learning is a subset of ML. \n",
    "#### ML \n",
    "- ML is a subset of AI \n",
    "- Let us assume we have a data from a recent expedition (Relation between height and weight of penguine), the each data point is plotted in the graph. \n",
    "  - We got height and weight and put it up against the graph and created a pattern, now next time if we get one data of height we can predict what the weight  will be , just an example. Looking for a trend line.. Which is called the training data point. \n",
    "##### Types of Models\n",
    "- We can create a model using the data point, Models are:\n",
    "  - **The Linear Regression** (Collecting the data of height and weight (X/Y axis) and creating a more predictable linear line )\n",
    "  - **The Logistic Regression**(Created for Yes or No scenarios)\n",
    "  - **Support Vector machine**. (Cluster information into catogories)\n",
    "  - **Decision Trees** (Decisioning) \n",
    "##### How ML Works\n",
    "- Every models start with the data, and choose an algorithm (What prediction that you need to make (example Use linear regression)), then take the data and alogirthm to train a mode. The training is computational expensive process. The output of this training is the model itself. Now we use this model using outside or new data to make predictions. \n",
    "- Adding extra dimention to the data, or more attributes to consider like, feather density, beak types, feet length.. We will endup drawing a multi dimensions of graph, The relationship makes it bit complicated for organic braib. But ML can do calculations using multi dimensional or all dimenstional large datasets. \n",
    "#### Deep Learning \n",
    "- Actually based on principle of organic brain, DataScientist has looked at the way the organic brain works and taken the inputs into the Deeplearning. The way Brain Neurons works with neural networks. The data scientist created *Ariticial Neurons* which takes input and outputs, and inside the neuron it has activation functions. \n",
    "- - it takes the input, process it and passes the outputs. \n",
    "- - Ariticial neural network: made up of many artifical neurons interconnected. \n",
    "- - Having the 100s of Artifical neurons interconnected each other in a network, is the structure of Deep learning. \n",
    "- - in the same way the organic neron passes the signal, the artificial neurons passes the inputs, and configure how the input has to be processed and there is a output layer to determine the outcome. \n",
    "- - Example: If an image has to be distinguished wheather it is a Kiwi or Penguin, it needs an output called Penguin or Kiwi. The input layer passes the input of image, processed by the middle layer and gets the correct output.\n",
    "##### Neural networks: evaluation and optimization\n",
    "\n",
    "\n",
    "#### Machine Learning concepts\n",
    "##### Lifecycle \n",
    "1. **Collect Data** \n",
    "    - Any kind of raw data we collect, Like from filed measuring penguins, image data, machine data \n",
    "2. **Process Data** \n",
    "    - The data that we collected from the filed will have all forms of data. \n",
    "    - Get the data into a format where the algorithm understands.\n",
    "    - Then organize the data. (Creating data in to a table)\n",
    "    - Add features and labels into the data. \n",
    "    - Reduce the amount of extranous data.\n",
    "    - Encoding and formatting.\n",
    "3. **Split the Data**\n",
    "    - *Training Data*\n",
    "      - This is where we create the model.\n",
    "      - How to choose the parameter and algorithm to train the data \n",
    "      - This is the reason why splitting the data is important. The training data directly influences the model.\n",
    "    - *validation Data*\n",
    "      - Training mechanism uses the validation data to see how acurate the model is generating. \n",
    "    - *Testing data*\n",
    "      - Testing Data is not used any time during the training.\n",
    "4. **Train a Model**\n",
    "5. **Test the model**\n",
    "    - The data that collected during the Training data will be used to see how well the model fits. \n",
    "6. **Deploy the model**\n",
    "    - Deploying model is very much of infra build.\n",
    "7. **Infer (Make the predictions)**\n",
    "    - Giving Real world unlabelled data, giving it to the model and asking it to label.\n",
    "    - This means making predictions. \n",
    "8. **Improve (How to improve the model in real time)**\n",
    "    - As part of improvement the whole cycle starts from Step 3: Split Data\n",
    "\n",
    "##### Algorithms\n",
    " How the algorithms learns themselves?\n",
    "- **Supervised Learning**\n",
    "  - The original labelled data is collected using ML is used to predict or infern. \n",
    "  - Supervising, Labelled data is basics of supervised learning. \n",
    "  - We have told the ML algorithm, this is what it looks like when some one likes penguin, so that it can predict when someone likes the penguins. \n",
    "  - The Linear regression method used is another way of Supervised learning. \n",
    "  - When we have samples of labelled data, to give that for the machine learning model so that it can determine the new data and make inference.\n",
    "  - Given a new input X, predict the right output y\n",
    "  - Given examples of stars and galaxies, identify new objects in the sky\n",
    "  - Learn a model from labeled training data, then make predictions\n",
    "  - Supervised: we know the correct/desired outcome (label)\n",
    "  -  Subtypes: classification (predict a class) and regression (predict a numeric value)\n",
    "  - Most supervised algorithms that we will see can do both\n",
    "  - *Classification*\n",
    "    - Predict a class label (category), discrete and unordered\n",
    "    - Can be binary (e.g. spam/not spam) or multi-class (e.g. letter recognition)\n",
    "    - Many classifiers can return a confidence per class\n",
    "    - The predictions of the model yield a decision boundary separating the classes\n",
    "    - Refer code 1\n",
    "\n",
    "- **Unsuperviosed Learning**\n",
    "  - Looking for patterns in the data, in which we dont necessarly see a pattern. \n",
    "  - Imagine we have a data set with scores in x and y axis and there is no relation for the data.\n",
    "    - - Find a relation ship on data that organic brain necesarily dont know the relation. \n",
    "  - It is best used when many dimensions are available in the data. \n",
    "  - Unlabeled data, or data with unknown structure\n",
    "  - Explore the structure of the data to extract information\n",
    "  - Many types, we’ll just discuss two.\n",
    "  - **Clustering** \n",
    "    - Organize information into meaningful subgroups (clusters)\n",
    "    - Objects in cluster share certain degree of similarity (and dissimilarity to other clusters)\n",
    "    - Example: distinguish different types of customers\n",
    "  - **Dimensionality reduction**\n",
    "    - Data can be very high-dimensional and difficult to understand, learn from, store,…\n",
    "    - Dimensionality reduction can compress the data into fewer dimensions, while retaining most of the information\n",
    "    - Contrary to feature selection, the new features lose their (original) meaning\n",
    "    - The new representation can be a lot easier to model (and visualize)\n",
    "    - Used in unsupervised machine learning type.\n",
    "\n",
    "\n",
    "- **Semi Supervised Learning**\n",
    "  - learn a model from (few) labeled and (many) unlabeled examples\n",
    "\n",
    "- **Reinforcement Learning**\n",
    "  - It is used in robotics and automation. \n",
    "  - For example, use reward machanism, to train the model. For example, a robot is asked to pickup pengiun and if it picks wrong one we give a negative reward. The model try to get more reward. \n",
    "  - AWS Deepracer uses reinforcement learning. \n",
    "  - Develop an agent that improves its performance based on interactions with the environment\n",
    "  - Example: games like Chess, Go,…\n",
    "  - Search a (large) space of actions and states\n",
    "  - Reward function defines how well a (series of) actions works\n",
    "  - Learn a series of actions (policy) that maximizes reward through exploration\n",
    "\n",
    "## Learning = Representation + evaluation + optimization\n",
    "All machine learning algorithms consist of 3 components:\n",
    "\n",
    "- **Representation**: \n",
    "  A model must be represented in a formal language that the computer can handle\n",
    "  Defines the ‘concepts’ it can learn, the hypothesis space\n",
    "  E.g. a decision tree, neural network, set of annotated data points\n",
    "- **Evaluation**: \n",
    "  An internal way to choose one hypothesis over the other Objective function, scoring function, loss function\n",
    "  E.g. Difference between correct output and predictions\n",
    "\n",
    "- **Optimization**: \n",
    "  An efficient way to search the hypothesis space\n",
    "  Start from simple hypothesis, extend (relax) if it doesn’t fit the data\n",
    "  Start with initial set of model parameters, gradually refine them\n",
    "  Many methods, differing in speed of learning, number of optima,…\n",
    "A powerful/flexible model is only useful if it can also be optimized efficiently\n",
    "\n",
    "##### Optimization of Algorithms\n",
    "- In a linear regression which line fits the model best? \n",
    "It uses Gradient Descent method..\n",
    "###### Gradiant decent\n",
    "  - Imagine you would like to get a linear regression with the best possible method, or in other words optimal prediction line in the Pengiun's height and weight Map. How do we do it?\n",
    "  - We can measure difference between the points and line, and square the value and find the difference (Sum of the square residual)\n",
    "  - This is repated until we find the least residual and uses the value to find the correct line regression.\n",
    "  - The minumum slop of the parabolic graph is calculated with gradient descent. It performs a step by step calculation of gradiant and finds the bottom of the graph.The step size has to be right sized for the algorithm, as each step size determines how long the calculation runs, If its too short will take time. \n",
    "  - For a double dip parabolic graph botton line, it will have a different outcome, as the initial dip might not be a correct one, hence it requires a trail and error to find the best outcome. \n",
    "\n",
    "###### Regularization \n",
    "- Consider the linear graph of Height and Weight of Penguin, when it comes to real world data it might not fit very well. And we apply regularization to fit the generalized data better. \n",
    "- We dont add a regularization to the model, unless it is found not doing correct. \n",
    "Types of Regularization\n",
    "  - L1 Regularization \n",
    "  - L2 Rigde regularization\n",
    "We apply regularization when the model is overfit. \n",
    "\n",
    "###### Hyperparameters\n",
    "- Are external parameters, we can set when initiating the training Job. \n",
    "- Paramters are internal to the algorithms but Hperperameters are external. \n",
    "- Three different types of Hyperparameters\n",
    "Learning rate, Epochs and Batch size. \n",
    "- **Learning rates**\n",
    "  - Remember the Gradiant descent in which we learned about calculating the descent for line or parabolic graphs?\n",
    "  - Determies the size of the step taken during gradient descent optimization\n",
    "  - Set between Zero and One. \n",
    "- **Batch size**\n",
    "  - Number of samples used to train at any one time. \n",
    "  - It could be all of the data, or some of the data. \n",
    "  - It depends on the size of infrastructure you have. \n",
    "  - If you have multiple servers, make sure the mini batch size is set and it spreads the loads across the multiple servers\n",
    "- **Epochs** \n",
    "  - Are the number of times the algorithm will process the training data. \n",
    "  - Each time the algorithm passes through the data the intension is that algorithm creates more acurate model. \n",
    "  - Each epoch has one or more batches. \n",
    "\n",
    "###### Cross Validation\n",
    "- When we split the data the validation data is optional. \n",
    "- Imagine we have a dataset, we have it split for training data, validation data and testing. \n",
    "- The validation data is used by model to tweak the hyperparameters.\n",
    "- We split the data into training and testing data, and with cross validation we split the training data into multiple validation data and use it for training. All the training data is used for training and validation. This is called cross validation. \n",
    "\n",
    "##### Overfitting and Underfitting\n",
    "- It’s easy to build a complex model that is 100% accurate on the training data, but very bad on new data\n",
    "- **Overfitting**: building a model that is too complex for the amount of data you have\n",
    "  - You model peculiarities in your training data (noise, biases,…)\n",
    "  - Solve by making model simpler (regularization), or getting more data\n",
    "  - Most algorithms have hyperparameters that allow regularization\n",
    "- **Underfitting**: building a model that is too simple given the complexity of the data\n",
    "  - Use a more complex model\n",
    "- There are techniques for detecting overfitting (e.g. bias-variance analysis). More about that later\n",
    "- You can build ensembles of many models to overcome both underfitting and overfitting\n",
    "- There is often a sweet spot that you need to find by optimizing the choice of algorithms and hyperparameters, or using more data.\n",
    "  Example: regression using polynomial functions\n",
    "\n",
    "### Better data representations, better models\n",
    "- Algorithm needs to correctly transform the inputs to the right outputs\n",
    "- A lot depends on how we present the data to the algorithm\n",
    "- Transform data to better representation (a.k.a. encoding or embedding)\n",
    "- Can be done end-to-end (e.g. deep learning) or by first ‘preprocessing’ the data (e.g. feature selection/generation)\n",
    "\n",
    "## Building machine learning systems\n",
    "\n",
    "A typical machine learning system has multiple components, which we will cover in upcoming lectures:\n",
    "\n",
    "- **Preprocessing**: \n",
    "  - Raw data is rarely ideal for learning\n",
    "  - Feature scaling: bring values in same range\n",
    "  - Encoding: make categorical features numeric\n",
    "  - Discretization: make numeric features categorical\n",
    "  - Label imbalance correction (e.g. downsampling)\n",
    "  - Feature selection: remove uninteresting/correlated features\n",
    "  - Dimensionality reduction can also make data easier to learn\n",
    "  - Using pre-learned embeddings (e.g. word-to-vector, image-to-vector)\n",
    "\n",
    "- **Learning and evaluation**\n",
    "\n",
    "        Every algorithm has its own biases\n",
    "\n",
    "        No single algorithm is always best\n",
    "\n",
    "        Model selection compares and selects the best models\n",
    "\n",
    "            Different algorithms, different hyperparameter settings\n",
    "\n",
    "        Split data in training, validation, and test sets\n",
    "\n",
    "    Prediction\n",
    "\n",
    "        Final optimized model can be used for prediction\n",
    "\n",
    "        Expected performance is performance measured on independent test set\n",
    "\n",
    "    Together they form a workflow of pipeline\n",
    "\n",
    "    There exist machine learning methods to automatically build and tune these pipelines\n",
    "\n",
    "    You need to optimize pipelines continuously\n",
    "\n",
    "        Concept drift: the phenomenon you are modelling can change over time\n",
    "\n",
    "        Feedback: your model’s predictions may change future data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code 1: Supervised prediction example\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "# create a synthetic dataset\n",
    "X1, y1 = make_moons(n_samples=70, noise=0.2, random_state=8)\n",
    "\n",
    "# Train classifiers\n",
    "lr = LogisticRegression().fit(X1, y1)\n",
    "svm = SVC(kernel='rbf', gamma=2, probability=True).fit(X1, y1)\n",
    "knn = KNeighborsClassifier(n_neighbors=3).fit(X1, y1)\n",
    "\n",
    "# Plotting\n",
    "@interact\n",
    "def plot_classifier(classifier=[lr,svm,knn]):  \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12*fig_scale, 4*fig_scale))\n",
    "    mglearn.tools.plot_2d_separator(\n",
    "        classifier, X1, ax=axes[0], alpha=.4, cm=mglearn.cm2)\n",
    "    scores_image = mglearn.tools.plot_2d_scores(\n",
    "        classifier, X1, ax=axes[1], alpha=.5, cm=mglearn.ReBl, function='predict_proba')\n",
    "    for ax in axes:\n",
    "        mglearn.discrete_scatter(X1[:, 0], X1[:, 1], y1,\n",
    "                                 markers='.', ax=ax)\n",
    "        ax.set_xlabel(\"Feature 0\")\n",
    "        ax.set_ylabel(\"Feature 1\", labelpad=0)\n",
    "        ax.tick_params(axis='y', pad=0)\n",
    "\n",
    "    cbar = plt.colorbar(scores_image, ax=axes.tolist())\n",
    "    cbar.set_label('Predicted probability', rotation=270, labelpad=6)\n",
    "    cbar.set_alpha(1)\n",
    "    cbar.draw_all() \n",
    "    axes[0].legend([\"Class 0\", \"Class 1\"], ncol=4, loc=(.1, 1.1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    plot_classifier(classifier=svm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
