{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI ML notes\n",
    "#### AI\n",
    "- AI in reality is in place from mid 60s, like spam filters, AVs \n",
    "- The compute power needed to make the AI to work was not available before 90s, but by 2011s advancement of computer power the AI is possible.\n",
    "- AI is very broad object.\n",
    "- - ML is a subset of AI \n",
    "- - - Deep learning is a subset of ML. \n",
    "#### ML \n",
    "- ML is a subset of AI \n",
    "- Let us assume we have a data from a recent expedition (Relation between height and weight of penguine), the each data point is plotted in the graph. \n",
    "  - We got height and weight and put it up against the graph and created a pattern, now next time if we get one data of height we can predict what the weight  will be , just an example. Looking for a trend line.. Which is called the training data point. \n",
    "##### Types of Models\n",
    "- We can create a model using the data point, Models are:\n",
    "  - **The Linear Regression** (Collecting the data of height and weight (X/Y axis) and creating a more predictable linear line )\n",
    "  - **The Logistic Regression**(Created for Yes or No scenarios)\n",
    "  - **Support Vector machine**. (Cluster information into catogories)\n",
    "  - **Decision Trees** (Decisioning) \n",
    "##### How ML Works\n",
    "- Every models start with the data, and choose an algorithm (What prediction that you need to make (example Use linear regression)), then take the data and alogirthm to train a mode. The training is computational expensive process. The output of this training is the model itself. Now we use this model using outside or new data to make predictions. \n",
    "- Adding extra dimention to the data, or more attributes to consider like, feather density, beak types, feet length.. We will endup drawing a multi dimensions of graph, The relationship makes it bit complicated for organic braib. But ML can do calculations using multi dimensional or all dimenstional large datasets. \n",
    "#### Deep Learning \n",
    "- Actually based on principle of organic brain, DataScientist has looked at the way the organic brain works and taken the inputs into the Deeplearning. The way Brain Neurons works with neural networks. The data scientist created *Ariticial Neurons* which takes input and outputs, and inside the neuron it has activation functions. \n",
    "- - it takes the input, process it and passes the outputs. \n",
    "- - Ariticial neural network: made up of many artifical neurons interconnected. \n",
    "- - Having the 100s of Artifical neurons interconnected each other in a network, is the structure of Deep learning. \n",
    "- - in the same way the organic neron passes the signal, the artificial neurons passes the inputs, and configure how the input has to be processed and there is a output layer to determine the outcome. \n",
    "- - Example: If an image has to be distinguished wheather it is a Kiwi or Penguin, it needs an output called Penguin or Kiwi. The input layer passes the input of image, processed by the middle layer and gets the correct output.\n",
    "\n",
    "#### Machine Learning concepts\n",
    "##### Lifecycle \n",
    "1. **Collect Data** \n",
    "    - Any kind of raw data we collect, Like from filed measuring penguins, image data, machine data \n",
    "2. **Process Data** \n",
    "    - The data that we collected from the filed will have all forms of data. \n",
    "    - Get the data into a format where the algorithm understands.\n",
    "    - Then organize the data. (Creating data in to a table)\n",
    "    - Add features and labels into the data. \n",
    "    - Reduce the amount of extranous data.\n",
    "    - Encoding and formatting.\n",
    "3. **Split the Data**\n",
    "    - *Training Data*\n",
    "      - This is where we create the model.\n",
    "      - How to choose the parameter and algorithm to train the data \n",
    "      - This is the reason why splitting the data is important. The training data directly influences the model.\n",
    "    - *validation Data*\n",
    "      - Training mechanism uses the validation data to see how acurate the model is generating. \n",
    "    - *Testing data*\n",
    "      - Testing Data is not used any time during the training.\n",
    "4. **Train a Model**\n",
    "5. **Test the model**\n",
    "    - The data that collected during the Training data will be used to see how well the model fits. \n",
    "6. **Deploy the model**\n",
    "    - Deploying model is very much of infra build.\n",
    "7. **Infer (Make the predictions)**\n",
    "    - Giving Real world unlabelled data, giving it to the model and asking it to label.\n",
    "    - This means making predictions. \n",
    "8. **Improve (How to improve the model in real time)**\n",
    "    - As part of improvement the whole cycle starts from Step 3: Split Data\n",
    "\n",
    "##### Algorithms\n",
    " How the algorithms learns themselves?\n",
    "- **Supervised Learning**\n",
    "  - The original labelled data is collected using ML is used to predict or infern. \n",
    "  - Supervising, Labelled data is basics of supervised learning. \n",
    "  - We have told the ML algorithm, this is what it looks like when some one likes penguin, so that it can predict when someone likes the penguins. \n",
    "  - The Linear regression method used is another way of Supervised learning. \n",
    "  - When we have samples of labelled data, to give that for the machine learning model so that it can determine the new data and make inference.\n",
    "- **Unsuperviosed Learning**\n",
    "  - Looking for patterns in the data, in which we dont necessarly see a pattern. \n",
    "  - Imagine we have a data set with scores in x and y axis and there is no relation for the data.\n",
    "    - - Find a relation ship on data that organic brain necesarily dont know the relation. \n",
    "  - It is best used when many dimensions are available in the data. \n",
    "- **Reinforcement Learning**\n",
    "  - It is used in robotics and automation. \n",
    "  - For example, use reward machanism, to train the model. For example, a robot is asked to pickup pengiun and if it picks wrong one we give a negative reward. The model try to get more reward. \n",
    "  - AWS Deepracer uses reinforcement learning. \n",
    "##### Optimization of Algorithms\n",
    "- In a linear regression which line fits the model best? \n",
    "It uses Gradient Descent method..\n",
    "###### Gradiant decent\n",
    "  - Imagine you would like to get a linear regression with the best possible method, or in other words optimal prediction line in the Pengiun's height and weight Map. How do we do it?\n",
    "  - We can measure difference between the points and line, and square the value and find the difference (Sum of the square residual)\n",
    "  - This is repated until we find the least residual and uses the value to find the correct line regression.\n",
    "  - The minumum slop of the parabolic graph is calculated with gradient descent. It performs a step by step calculation of gradiant and finds the bottom of the graph.The step size has to be right sized for the algorithm, as each step size determines how long the calculation runs, If its too short will take time. \n",
    "  - For a double dip parabolic graph botton line, it will have a different outcome, as the initial dip might not be a correct one, hence it requires a trail and error to find the best outcome. \n",
    "\n",
    "###### Regularization \n",
    "- Consider the linear graph of Height and Weight of Penguin, when it comes to real world data it might not fit very well. And we apply regularization to fit the generalized data better. \n",
    "- We dont add a regularization to the model, unless it is found not doing correct. \n",
    "Types of Regularization\n",
    "  - L1 Regularization \n",
    "  - L2 Rigde regularization\n",
    "We apply regularization when the model is overfit. \n",
    "\n",
    "###### Hyperparameters\n",
    "- Are external parameters, we can set when initiating the training Job. \n",
    "- Paramters are internal to the algorithms but Hperperameters are external. \n",
    "- Three different types of Hyperparameters\n",
    "Learning rate, Epochs and Batch size. \n",
    "- **Learning rates**\n",
    "  - Remember the Gradiant descent in which we learned about calculating the descent for line or parabolic graphs?\n",
    "  - Determies the size of the step taken during gradient descent optimization\n",
    "  - Set between Zero and One. \n",
    "- **Batch size**\n",
    "  - Number of samples used to train at any one time. \n",
    "  - It could be all of the data, or some of the data. \n",
    "  - It depends on the size of infrastructure you have. \n",
    "  - If you have multiple servers, make sure the mini batch size is set and it spreads the loads across the multiple servers\n",
    "- **Epochs** \n",
    "  - Are the number of times the algorithm will process the training data. \n",
    "  - Each time the algorithm passes through the data the intension is that algorithm creates more acurate model. \n",
    "  - Each epoch has one or more batches. \n",
    "\n",
    "###### Cross Validation\n",
    "- When we split the data the validation data is optional. \n",
    "- Imagine we have a dataset, we have it split for training data, validation data and testing. \n",
    "- The validation data is used by model to tweak the hyperparameters.\n",
    "- We split the data into training and testing data, and with cross validation we split the training data into multiple validation data and use it for training. All the training data is used for training and validation. This is called cross validation. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
